data = data[data$h10_g4 < 1990, ]
dim(data)
data = data %>% filter(p1003_12 %in% c(1,2,4,5))
dim(data)
data$p1003_12
sum(na.omit(data$y == 1))
sum(na.omit(data$y == 2))
sum(na.omit(data$y == 4))
sum(na.omit(data$y == 5))
data$y = data$p1003_12
data = subset( data, select = -c(p1003_12))
dim(data)
data = subset( data, select = -c(p1003_5,p1003_6,p1003_7,p1003_8,p1003_9,p1003_10,p1003_11))
dim(data)
data$y
sum(na.omit(data$y == 1))
sum(na.omit(data$y == 2))
sum(na.omit(data$y == 4))
sum(na.omit(data$y == 5))
str(data$y)
data$y[data$y == 2] = 1 # 변환
data$y[data$y == 5] = 4
data$y[data$y == 4] = 2
dim(data)
colSums(is.na(data))/7923 < 0.5
is.na(data)
colSums(is.na(data))
dim(data)
colSums(is.na(data))/7923 < 0.5
x = data[colSums(is.na(data))/7923 < 0.5] # na가 0.5 이상인 열은 제거
dim(x)
colSums(is.na(x))
str(x)
colSums(is.na(x)) #
x = subset( data, select = -c(h10_id,h10_ind,h10_sn,h10_merkey)) # 가구 패널 ID와 관련된 정보는 특별한 의미가 없다고 생각되어 제거
str(x)
x = subset( data, select = -c(X...h10_id,h10_ind,h10_sn,h10_merkey)) # 가구 패널 ID와 관련된 정보는 특별한 의미가 없다고 생각되어 제거
str(x)
dim(x)
x = data[colSums(is.na(data))/7923 < 0.5] # na가 0.5 이상인 열은 제거
dim(x)
colSums(is.na(x))
x = subset( x, select = -c(X...h10_id,h10_ind,h10_sn,h10_merkey)) # 가구 패널 ID와 관련된 정보는 특별한 의미가 없다고 생각되어 제거
dim(x)
summary(colSums(is.na(x)))
sort(colSums(is.na(x)))
x$h1017_2
sort(colSums(is.na(x)))[1:10]
sort(colSums(is.na(x)),)[1:10]
sort
?sort
sort(colSums(is.na(x)),decreasing = T)[1:10]
sort(colSums(is.na(x)),decreasing = T) # 결측치 값들을 크기 순으로 정렬
sort(colSums(is.na(x)),decreasing = T)[1:10]
?preProcess
library(caret)
?preProcess
imp.x = preProcess(x, method = c("bagImpute"))
dim(imp.x)
imp.x
is.na(x)
sum(is.na(x))
imp.x
dim(x)
imp.x1 = preProcess(x, method = c("knnImpute"))
sum(is.na(x$h10_flag))
sum(is.na(x$p10_cp))
sum(is.na(x))
is.na(x)
sort(colSums(is.na(x)),decreasing = T) # 결측치 값들을 크기 순으로 정렬
sort(colSums(is.na(x)),decreasing = T)[1:10]
sum(is.na(x$h10_inc7_2))
imp.x1 = preProcess(x, method = c("knnImpute"))
x$h10_cin
imp.x
library(caret)
data(airquality); summary(airquality)
imp.1 <- preProcess(airquality, method=c("knnImpute"))
imp.1
imp.2 <- predict(imp.1, airquality); summary(airquality)
#install.packages('RANN')
library(RANN)
install.packages('RANN')
imp.2 <- predict(imp.1, airquality); summary(airquality)
imp.2
imp.2 <- predict(imp.x, x)
x
imp.x
x = as.data.frame(x)
imp.2 <- predict(imp.x, x)
dim(x)
str(x)
imp.2 <- predict(imp.x, x)
data$h1013_4aq15
x = data[colSums(is.na(data))/7923 < 0.5] # na가 0.5 이상인 열은 제거
dim(x)
x = subset( x, select = -c(X...h10_id,h10_ind,h10_sn,h10_merkey,h1013_4aq15)) # 가구 패널 ID와 관련된 정보는 특별한 의미가 없다고 생각되어 제거
sort(colSums(is.na(x)),decreasing = T)[1:10]
imp.x = preProcess(x, method = c("bagImpute")) # 결측값을 bagImputer로 채움
data$h1013_4aq15
imp.2 <- predict(imp.x, x)
dim(imp.2)
sum(is.na(imp.2))
library(earth)
dummy.1 = dummyVars(y~,. data = imp.2)
dummy.1 = dummyVars(y~., data = imp.2)
head(dummy.1)
str(dummy.1)
dummy.2 = predict(dummy.1, newdata = imp.2)
head(dummy.2)
str(dummy.2)
dim(dummy.2)
dim(imp.2)
sum(is.na(imp.2))
set.seed(1996)
intrain<-createDataPartition(y=imp.2$y, p=0.7, list=FALSE)
train<-df[intrain, ]
train<-imp.2[intrain, ]
test<-imp.2[-intrain, ]
treemod<-tree(y~. , data=train)
library(tree)
### decision tree를 이용한 분석
install.packages("tree")
library(tree)
treemod<-tree(y~. , data=train)
plot(treemod)
set.seed(1996)
intrain<-createDataPartition(y=imp.2$y, p=0.7, list=FALSE)
train<-imp.2[intrain, ]
test<-imp.2[-intrain, ]
library(tree)
treemod<-tree(y~. , data=train)
plot(treemod)
text(treemod)
cv.trees<-cv.tree(treemod, FUN=prune.misclass ) # for classification decision tree
### decision tree를 이용한 분석
library(e1071)
plot(cv.trees)
cv.trees<-cv.tree(treemod, FUN=prune.misclass ) # for classification decision tree
library(rpart)
rpartmod<-rpart(y~. , data=train, method="class")
plot(rpartmod)
text(rpartmod)
printcp(rpartmod)
plotcp(rpartmod)
ptree<-prune(rpartmod, cp= rpartmod$cptable[which.min(rpartmod$cptable[,"xerror"]),"CP"])
plot(ptree)
text(ptree)
rpartpred<-predict(ptree, test, type='class')
confusionMatrix(rpartpred, test$AHD)
confusionMatrix(rpartpred, test$y)
rpartpred
test$y
dim(rpartpred)
rpartpred
ptree
rpartmod
dim(train)
confusionMatrix(rpartpred, test)
str(test$y)
test$y = as.factor(test$y)
confusionMatrix(rpartpred, test$y)
dim(test)
#### random forest
install.packages('randomForest')
library(randomForest)
forest_m = randomForest(y~., data = train)
train$y
train$y = as.factor(train$y)
rpartmod<-rpart(y~. , data=train, method="class")
plot(rpartmod)
text(rpartmod)
printcp(rpartmod)
plotcp(rpartmod)
ptree<-prune(rpartmod, cp= rpartmod$cptable[which.min(rpartmod$cptable[,"xerror"]),"CP"])
plot(ptree)
text(ptree)
rpartpred<-predict(ptree, test, type='class')
str(test$y)
test$y = as.factor(test$y)
dim(test)
confusionMatrix(rpartpred, test$y)
#### random forest
library(randomForest)
forest_m = randomForest(y~., data = train)
forest_m$predicted
forest_m$importance
forest_m
forest_m$importance
sort(forest_m$importance)
forest_m$importance[forest_m$importance < 0.000000000001]
forest_m$mtry
forest_m$ntree
train = subset( train, select = -c(p1002_8aq13,p1002_8aq14,p1002_8aq11,p1002_8aq8,p1002_8aq9,p10_cp,h1010_aq17,h1008_6aq1,h1006_39,h10_flag))
test = subset( test, select = -c(p1002_8aq13,p1002_8aq14,p1002_8aq11,p1002_8aq8,p1002_8aq9,p10_cp,h1010_aq17,h1008_6aq1,h1006_39,h10_flag))
dim(train)
forest_m = randomForest(y~., data = train)
forest_m
forest_m$predicted
forest_m$importance
forest_m$mtry
forest_m$ntree
prd_v <- predict(forest_m, newdata = test, type = 'class')
sum(prd_v == test$y) / nrow(test) * 100
sum(prd_v2 == train$y) / nrow(train) * 100
prd_v2 <- predict(forest_m, newdata = train, type = 'class')
sum(prd_v2 == train$y) / nrow(train) * 100
layout(matrix(c(1,2),nrow=1),width=c(4,1))
par(mar=c(5,4,4,0)) # 오른쪽 마진 제거
plot(forest_m)
par(mar=c(5,0,4,2)) # 왼쪽 마진 제거
plot(c(0,1),type="n", axes=F, xlab="", ylab="")
legend("top", colnames(forest_m$err.rate),col=1:4,cex=0.8,fill=1:4)
ntree_test <- c()
for (i in 1:1000) {
m <- randomForest(y ~ . , data=train, ntree = i)
val_var <- predict(m, newdata=test, type="class")
ntree_test <- c(ntree_test, sum(val_var == test$y)/nrow(test) * 100)
}
ntree_test
# train data에 대한 score
ntree_train <- c()
for (i in 1:1000) {
m <- randomForest(y ~ . , data=train, ntree = i)
val_var <- predict(m, newdata=train, type="class")
ntree_train <- c(ntree_train, sum(val_var == train$y)/nrow(train) * 100)
}
ntree_train
# 2) mtry 변화
# test data에 대한 score
mtry_test <- c()
for (i in 1:(length(train)-1)) {
m <- randomForest(y ~ . , data=train, mtry = i)
val_var <- predict(m, newdata=test, type="class")
mtry_test <- c(mtry_test, sum(val_var == test$y)/nrow(test) * 100)
}
mtry_test
# train data에 대한 score
mtry_train <- c()
for (i in 1:(length(train)-1)) {
m <- randomForest(y ~ . , data=train, mtry = i)
val_var <- predict(m, newdata=train, type="class")
mtry_train <- c(mtry_train, sum(val_var == train$y)/nrow(train) * 100)
}
mtry_train
plot(1:length(ntree_test), ntree_test, type = 'l', xlab = 'ntree(size of tree)', col='red', ylim = c(95,100))
lines(1:length(ntree_train), ntree_train, type = 'l')
plot(1:length(mtry_test)-1, mtry_test, type = 'b', xlab = 'mtry', col='red', ylim = c(95,100))
lines(1:length(mtry_train)-1, mtry_train, type = 'b')
install.packages('lightgbm')
library(lightgbm)
data(agaricus.train, package = "lightgbm")
train <- agaricus.train
train
train<-imp.2[intrain, ]
test<-imp.2[-intrain, ]
train$y = as.factor(train$y)
test$y = as.factor(test$y)
train = subset( train, select = -c(p1002_8aq13,p1002_8aq14,p1002_8aq11,p1002_8aq8,p1002_8aq9,p10_cp,h1010_aq17,h1008_6aq1,h1006_39,h10_flag))
test = subset( test, select = -c(p1002_8aq13,p1002_8aq14,p1002_8aq11,p1002_8aq8,p1002_8aq9,p10_cp,h1010_aq17,h1008_6aq1,h1006_39,h10_flag))
dim(train)
dtrain <- lgb.Dataset(train, label = train$y)
train[-1]
train[,-1]
train[-1,]
?lgb.cv
params <- list(objective = "classification", metric = "l2")
valids <- list(test = dtest)
dtest <- lgb.Dataset.create.valid(dtrain, test, label = test$y)
params <- list(objective = "classification", metric = "l2")
valids <- list(test = dtest)
model <- lgb.train(
params = params
, data = dtrain
, nrounds = 5L
, valids = valids
, min_data = 1L
, learning_rate = 1.0
)
set.seed(1996)
intrain<-createDataPartition(y=imp.2$y, p=0.7, list=FALSE)
train<-imp.2[intrain, ]
test<-imp.2[-intrain, ]
### decision tree를 이용한 분석
library(e1071)
library(tree)
library(rpart)
train$y = as.factor(train$y)
test$y = as.factor(test$y)
train = subset( train, select = -c(p1002_8aq13,p1002_8aq14,p1002_8aq11,p1002_8aq8,p1002_8aq9,p10_cp,h1010_aq17,h1008_6aq1,h1006_39,h10_flag))
test = subset( test, select = -c(p1002_8aq13,p1002_8aq14,p1002_8aq11,p1002_8aq8,p1002_8aq9,p10_cp,h1010_aq17,h1008_6aq1,h1006_39,h10_flag))
ntree_test <- c()
for (i in 300:500) {
m <- randomForest(y ~ . , data=train, ntree = i)
val_var <- predict(m, newdata=test, type="class")
ntree_test <- c(ntree_test, sum(val_var == test$y)/nrow(test) * 100)
}
ntree_test
# train data에 대한 score
ntree_train <- c()
for (i in 300:500) {
m <- randomForest(y ~ . , data=train, ntree = i)
val_var <- predict(m, newdata=train, type="class")
ntree_train <- c(ntree_train, sum(val_var == train$y)/nrow(train) * 100)
}
ntree_train
# 2) mtry 변화
# test data에 대한 score
mtry_test <- c()
for (i in 1:(length(train)-1)) {
m <- randomForest(y ~ . , data=train, mtry = i)
val_var <- predict(m, newdata=test, type="class")
mtry_test <- c(mtry_test, sum(val_var == test$y)/nrow(test) * 100)
}
mtry_test
# train data에 대한 score
mtry_train <- c()
for (i in 1:(length(train)-1)) {
m <- randomForest(y ~ . , data=train, mtry = i)
val_var <- predict(m, newdata=train, type="class")
mtry_train <- c(mtry_train, sum(val_var == train$y)/nrow(train) * 100)
}
mtry_train
dim(data)
setwd('C:/Users/신상훈/Documents/GitHub/data-mining-project/데이터 및 조사설계서 (1)/데이터 및 조사설계서')
Sys.setlocale(category = "LC_CTYPE", locale = "C")
data = read.csv("데이터.csv",header = TRUE)
Sys.setlocale(category = 'LC_ALL',locale='korean')
library(dplyr)
library(caret)
library(RANN)
library(earth)
dim(data)
data = data[data$h10_g4 < 1990, ] # 90년대 이전 출생자 뽑기
dim(data)
dim(data)
data = data %>% filter(p1003_12 %in% c(1,2,4,5)) # p1003_12에서 1,2,4,5만 뽑기
dim(data)
data$p1003_12
data$y = data$p1003_12 # y로 변환
data = subset( data, select = -c(p1003_5,p1003_6,p1003_7,p1003_8,p1003_9,p1003_10,p1003_11))
dim(data)
data$y = data$p1003_12 # y로 변환
data = subset( data, select = -c(p1003_12))
dim(data)
data$y
data = subset( data, select = -c(p1003_5,p1003_6,p1003_7,p1003_8,p1003_9,p1003_10,p1003_11))
dim(data)
data$y
data = as.data.frame(data)
dim(data)
str(data$y)
# 개수 확인
sum(na.omit(data$y == 1))
sum(na.omit(data$y == 2))
sum(na.omit(data$y == 4))
sum(na.omit(data$y == 5))
colSums(is.na(data)) # na가 극단적으로 많은 값과 적은 값들로 이루어져 있으므로 비율이 0.5 기준으로 이상인 값들의 열을 제거
colSums(is.na(data))/7923 < 0.5
data$y[data$y == 2] = 1 # 변환
data$y[data$y == 5] = 4
data$y[data$y == 4] = 2
dim(data)
colSums(is.na(data)) # na가 극단적으로 많은 값과 적은 값들로 이루어져 있으므로 비율이 0.5 기준으로 이상인 값들의 열을 제거
colSums(is.na(data))/7923 < 0.5
x = data[colSums(is.na(data))/7923 < 0.5] # na가 0.5 이상인 열은 제거
dim(x)
colSums(is.na(x))
str(x)
sort(colSums(is.na(x)),decreasing = T) # 결측치 값들을 크기 순으로 정렬
imp.x = preProcess(x, method = c("bagImpute")) # 결측값을 bagImputer로 채움
sum(is.na(x))
imp.2 <- predict(imp.x, x)
dim(imp.2)
sum(is.na(imp.2))
set.seed(1996)
intrain<-createDataPartition(y=imp.2$y, p=0.7, list=FALSE)
train<-imp.2[intrain, ]
test<-imp.2[-intrain, ]
#### random forest
library(randomForest)
forest_m = randomForest(y~., data = train)
forest_m
forest_m$importance
forest_m$mtry
forest_m$ntree
x$y = as.factor(x$y)
imp.x = preProcess(x, method = c("bagImpute")) # 결측값을 bagImputer로 채움
imp.2 <- predict(imp.x, x)
sum(is.na(imp.2))
dim(data)
dim(x)
colSums(is.na(x))
x = subset( x, select = -c(X...h10_id,h10_ind,h10_sn,h10_merkey,h1013_4aq15)) # 가구 패널 ID와 관련된 정보는 특별한 의미가 없다고 생각되어 제거, h1013_4aq15 값이 없으므로 제거
dim(x)
str(x)
sort(colSums(is.na(x)),decreasing = T) # 결측치 값들을 크기 순으로 정렬
sort(colSums(is.na(x)),decreasing = T)[1:10]
x$y = as.factor(x$y)
x$h1013_4aq15
imp.x = preProcess(x, method = c("bagImpute")) # 결측값을 bagImputer로 채움
imp.2 <- predict(imp.x, x)
sum(is.na(imp.2))
intrain<-createDataPartition(y=imp.2$y, p=0.7, list=FALSE)
train<-imp.2[intrain, ]
test<-imp.2[-intrain, ]
#### random forest
library(randomForest)
forest_m = randomForest(y~., data = train)
forest_m
forest_m$predicted
forest_m$importance
forest_m$mtry
forest_m$ntree
sort(forest_m$importance)
train = subset( train, select = -c(p1002_8aq13,p1002_8aq14,p1002_8aq11,p1002_8aq8,p1002_8aq9,p10_cp,h1010_aq17,h1008_6aq1,h1006_39,h10_flag))
test = subset( test, select = -c(p1002_8aq13,p1002_8aq14,p1002_8aq11,p1002_8aq8,p1002_8aq9,p10_cp,h1010_aq17,h1008_6aq1,h1006_39,h10_flag))
dim(train)
forest_m = randomForest(y~., data = train)
forest_m
forest_m$predicted
forest_m$importance
forest_m$mtry
forest_m$ntree
prd_v <- predict(forest_m, newdata = test, type = 'class')
sum(prd_v == test$y) / nrow(test) * 100
prd_v2 <- predict(forest_m, newdata = train, type = 'class')
sum(prd_v2 == train$y) / nrow(train) * 100
##### graph
layout(matrix(c(1,2),nrow=1),width=c(4,1))
par(mar=c(5,4,4,0)) # 오른쪽 마진 제거
plot(forest_m)
par(mar=c(5,0,4,2)) # 왼쪽 마진 제거
plot(c(0,1),type="n", axes=F, xlab="", ylab="")
legend("top", colnames(forest_m$err.rate),col=1:4,cex=0.8,fill=1:4)
forest_m1 = randomForest(y~., data = train, mtry=20, ntree = 300)
prd_v <- predict(forest_m1, newdata = test, type = 'class')
prd_v
yhat_rf = predict(forest_m1,newdata = test,type = 'prob' )
binomial_deviance(train$y, yhat_rf)
pred_rf = prediction( yhat_rf,train$y)
install.packages('prediction')
library(prediction)
pred_rf = prediction( yhat_rf,train$y)
yhat_rf
forest_m1
270+4932
4932/5202
TN = 311
FN = 34
FP = 270
TP = 4932
Sensitivity = TP / (FP+TP)
Sensitivity
precision = TP / (FP+TP)
precision
Sensitivity = TP/(FN+TP)
Sensitivity
ACCURACY = (TP + TN)/(TN + FN + TP + FP)
ACCURACY
### decision tree를 이용한 분석
library(e1071)
library(tree)
library(rpart)
rpartmod<-rpart(y~. , data=train, method="class")
rpartmod
plot(rpartmod)
text(rpartmod)
printcp(rpartmod)
plotcp(rpartmod)
plotcp(rpartmod)
ptree<-prune(rpartmod, cp= rpartmod$cptable[which.min(rpartmod$cptable[,"xerror"]),"CP"])
plot(ptree)
text(ptree)
ptree<-prune(rpartmod, cp= rpartmod$cptable[which.min(rpartmod$cptable[,"xerror"]),"CP"])
plot(ptree)
text(ptree)
rpartpred<-predict(ptree, test, type='class')
confusionMatrix(rpartpred, test$y)
precision
Sensitivity = TP/(FN+TP)
Sensitivity
ACCURACY = (TP + TN)/(TN + FN + TP + FP)
ACCURACY
Sensitivity = FP/(FN+TP)
Sensitivity
TN = 311
FN = 34
TP = 270
FP = 4932
precision = TP / (FP+TP)
precision
Sensitivity = FP/(FN+TP)
Sensitivity
ACCURACY = (TP + TN)/(TN + FN + TP + FP)
ACCURACY
TN = 311
FN = 34
FP = 270
TP = 4932
precision = TP / (FP+TP)
precision
Sensitivity = FP/(FN+TP)
Sensitivity
ACCURACY = (TP + TN)/(TN + FN + TP + FP)
ACCURACY
Specificity= TP / (FP+TP)
Specificity
Sensitivity = FP/(FN+TP)
Sensitivity
ACCURACY = (TP + TN)/(TN + FN + TP + FP)
ACCURACY
